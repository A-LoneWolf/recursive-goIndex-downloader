{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Recursive_GoIndex_Downloader_v2.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH7ZlsGKsQ0X",
    "colab_type": "text"
   },
   "source": [
    "## Recursive GoIndex Downloader by atlonxp\n",
    "\n",
    "**Features**\n",
    "*   Recursive crawler (**atlonxp**)\n",
    "*   Download all folders and files in a given url (**atlonxp**)\n",
    "*   Download all folders and files in in sub-folders (**atlonxp**)\n",
    "*   Adaptive delay in fetching url (**atlonxp**)\n",
    "*   Store folders/files directly to your Google Drive (**pankaj260**)\n",
    "*   Folders and files exclusion filters (**atlonxp**)\n",
    "*   Download queue supported (**atlonxp**)\n",
    "*   Auto-domain URL detection (**atlonxp**)\n",
    "*   API-based GoIndex crawler (**atlonxp**, **ifvv**)\n",
    "*   Parallel/Multiple files downloader (**atlonxp**)\n",
    "*   Auto-skip password-protected folders (**cxu-fork**)\n",
    "\n",
    "**Version 2** - API-based crawler with paralled files downloader\n",
    "\n",
    "\t21 Aprial 2020 (v2.3.1)\n",
    "\t---------------------\n",
    "\tWhile crawling, fetching might cause errors sometime due to some quick requests or server is \n",
    "\tbusy. This problem has caused the eror in getting a json, so we re-fetch the url again (up to \n",
    "\tMAX_RETRY_CRAWLING) or until we found key \"files\" in the return response. Once retries is \n",
    "\treached the maximum and the key \"files\" is not found, so we ignore this link (return [])\n",
    "\n",
    "\tAt the end, if you find there is failure, just re-run the download section again. Unless you \n",
    "\tset OVERWITE = TRUE, all files will be re-downloaded\n",
    "\n",
    "\t+ added MAX_RETRY_CRAWLING (v2.3)\n",
    "\t+ fixed FILE_EXISTING_CHECK (stupid) bug\n",
    "\t+ added failure-links download task\n",
    "\n",
    "\t20 Aprial 2020 (v2.2)\n",
    "\t---------------------\n",
    "\tSome sub-folders may be password-protected which will cause the error while crawling, so we \n",
    "\tskip this folder\n",
    "\n",
    "\t+ added auto-skip password-protected folder\n",
    "\n",
    "\t17 April 2020 (v2.1)\n",
    "    ---------------------\n",
    "\t+ fixed URL duplicated when crawling\n",
    "\t+ added search key 'files' function\n",
    "\n",
    "\t16 April 2020 (v2.0)\n",
    "    ---------------------\n",
    "\t+ crawler_v2:\n",
    "\t\t* API-based GoIndex crawler\n",
    "\t\t* Collecting all urls to be downloaded\n",
    "\t+ parallel downloader\n",
    "\t\t* TDQM progress bar"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SHZKD2eIrSWC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Mounting Google Drive, ignore this section if you don't want to \n",
    "# save on your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ei98hPvrrlNb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Install dependencies\n",
    "!pip install requests tqdm"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ki6wS0MCrnqC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Import dependencies\n",
    "\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from urllib import parse\n",
    "\n",
    "import requests\n",
    "import tqdm"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ew87GUTHUu2p",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "SHOW_DOWNLOAD_PROGRESS = False\n",
    "OVERWRITE = True\n",
    "\n",
    "MIN_DELAY = 3\n",
    "MAX_DELAY = 5\n",
    "MAX_RETRY_CRAWLING = 5\n",
    "\n",
    "def check_exclusion(name, exclusions):\n",
    "    for exc in exclusions:\n",
    "        if exc in name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find(key, dictionary):\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            for result in find(key, v):\n",
    "                yield result\n",
    "        elif isinstance(v, list):\n",
    "            for d in v:\n",
    "                for result in find(key, d):\n",
    "                    yield result\n",
    "\n",
    "\n",
    "def crawler_v2(url, downloading_dict, path, level, exclusions, verbose=False):\n",
    "    # let slow down a bit\n",
    "    sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "\n",
    "    url = parse.urlparse(url)\n",
    "    print(url.geturl())\n",
    "\n",
    "    try:\n",
    "        response_text = ''\n",
    "        retry = 0\n",
    "        while 'files' not in response_text:\n",
    "            retry += 1\n",
    "            if retry > MAX_RETRY_CRAWLING:\n",
    "                break\n",
    "            if retry > 1:\n",
    "                print('retry #{}'.format(retry), url.geturl())\n",
    "                sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "            response = requests.post(url.geturl(), data={})\n",
    "            response_text = response.text\n",
    "        # print(response.text)\n",
    "        response_json = json.loads(response_text)\n",
    "    except JSONDecodeError:\n",
    "        sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "        print('- Data is missing! change a plan -')\n",
    "        print('- > use terminal CURL            -')\n",
    "        try:\n",
    "            response = os.popen(\"curl {} -d ''\".format(url.geturl())).read()\n",
    "            response_json = json.loads(response)\n",
    "        except Exception as e:\n",
    "            print('Nah, something went wrong!')\n",
    "            print(e.args())\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print('Nah, something went wrong!')\n",
    "        print(e.args())\n",
    "        return []\n",
    "\n",
    "    if type(response_json) == dict and 'error' in response_json.keys():\n",
    "        print('Skip: ', response_json)\n",
    "        return downloading_dict\n",
    "\n",
    "    files_dict = list(find('files', response_json))[0]\n",
    "\n",
    "    for file in files_dict:\n",
    "        name = file['name']\n",
    "\n",
    "        # if @name contains exclusion word, we ignore\n",
    "        if check_exclusion(name, exclusions):\n",
    "            continue\n",
    "\n",
    "        if 'folder' in file['mimeType']:\n",
    "            next_url = url.geturl() + parse.quote(name) + \"/\"\n",
    "            next_path = os.path.join(path, name)\n",
    "            downloading_dict = crawler_v2(next_url, downloading_dict, next_path, level + 1, exclusions, verbose)\n",
    "        else:\n",
    "            name = file['name']\n",
    "            if verbose:\n",
    "                print('  ' + name)\n",
    "            downloading_dict.append({\n",
    "                'folder': path,\n",
    "                'filename': name,\n",
    "                'filename_abs': os.path.join(path, name),\n",
    "                'size': file['size'],\n",
    "                'url': url.geturl() + parse.quote(name),\n",
    "            })\n",
    "\n",
    "    # print(json.dumps(downloading_dict, indent=2), end='\\n\\n')\n",
    "    return downloading_dict\n",
    "\n",
    "\n",
    "def download_agent(task, OVERWRITE=OVERWRITE):\n",
    "    if task is None:\n",
    "        return None\n",
    "\n",
    "    # Making multiple requests too quick can cause yourself banned, so let set random delay (1, 10)\n",
    "    sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "\n",
    "    folder = task['folder']\n",
    "    filename_abs = task['filename_abs']\n",
    "    url = task['url']\n",
    "\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(filename_abs) or OVERWRITE:\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code is not 200:\n",
    "            return task\n",
    "        with open(filename_abs, 'ab+') as f:\n",
    "            f.write(r.content)\n",
    "    return None"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nqwqZ8Qrr-6m",
    "colab_type": "code",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "MAX_DOWNLOAD_TASKS = 16\n",
    "exclusions = ['__MACOSX/']\n",
    "\n",
    "destination = \"/content/drive/My Drive/Knowledge/_Trainings/_download\"\n",
    "download_tasks = [\n",
    "    {\n",
    "        'folder': 'FrontEndMasters - Complete Intro to Containers',\n",
    "        'url': 'https://tutnetflix.mlwdl.workers.dev/FrontEndMasters%20-%20Complete%20Intro%20to%20Containers/'\n",
    "    },\n",
    "    {\n",
    "        'folder': 'test',\n",
    "        'url': 'https://gdrv.icu/0:/Star/宮崎あや/2013-2016/'\n",
    "    },\n",
    "]\n",
    "\n",
    "print('##################################')\n",
    "print('# Crawling all downloadable urls #')\n",
    "print('##################################', end='\\n\\n')\n",
    "tasks = []\n",
    "for task in download_tasks:\n",
    "    tasks += crawler_v2(task['url'], [], os.path.join(destination, task['folder']), 0, exclusions, verbose=False)\n",
    "    # print(json.dumps(tasks, indent=2), end='\\n\\n')\n",
    "\n",
    "# print(json.dumps(tasks, indent=2))\n",
    "print('\\nCollecting', len(tasks), 'is completed', end='\\n\\n')\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZjpVT5B_to7l",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print('##################################')\n",
    "print('# Downloading files and folders  #')\n",
    "print('##################################', end='\\n\\n')\n",
    "pool = multiprocessing.Pool(processes=MAX_DOWNLOAD_TASKS)  # Num of CPUs\n",
    "\n",
    "failures = []\n",
    "tasks_list = [task.get('filename') for task in tasks]\n",
    "with tqdm.tqdm(total=len(tasks)) as pbar:\n",
    "    for i, result in enumerate(pool.imap_unordered(download_agent, tasks)):\n",
    "        pbar.set_description('Downloading %s' % tasks_list[i])\n",
    "        failures.append(result)\n",
    "        pbar.update()\n",
    "\n",
    "failures = [failure for failure in failures if failure is not None]\n",
    "if len(failures) > 0:\n",
    "    print('\\n\\n##################################')\n",
    "    print('# Retry all {} failures          #'.format(len(failures)))\n",
    "    print('##################################')\n",
    "    with tqdm.tqdm(total=len(failures)) as pbar:\n",
    "        for i, result in enumerate(pool.imap_unordered(download_agent, failures)):\n",
    "            pbar.set_description('Downloading %s' % tasks_list[i])\n",
    "            if result is None:\n",
    "                del failures[i]\n",
    "            failures.append(result)\n",
    "            pbar.update()\n",
    "\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "\n",
    "print('\\nAll done, Voila!')\n",
    "\n",
    "print('\\nAll done, Voila!')"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}