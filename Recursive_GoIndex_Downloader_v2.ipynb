{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Recursive_GoIndex_Downloader_v2.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH7ZlsGKsQ0X",
    "colab_type": "text"
   },
   "source": [
    "## Recursive GoIndex Downloader by atlonxp\n",
    "\n",
    "This code was created and improved by adapting the code from pankaj260 https://colab.research.google.com/drive/1tmsLGuswIZIZ_oM35EMW8TbJ6pQPt1rY#scrollTo=3bCnUMUg_SoT&forceEdit=true&sandboxMode=true\n",
    "\n",
    "**Features**\n",
    "*   Recursive crawler (atlonxp)\n",
    "*   Download all folders and files in a given url (atlonxp)\n",
    "*   Download all folders and files in in sub-folders (atlonxp)\n",
    "*   Adaptive delay in fetching url (atlonxp)\n",
    "*   Store folders/files directly to your Google Drive (pankaj260)\n",
    "*   Folders and files exclusion filters\n",
    "*   Download queue supported\n",
    "*   Auto-domain URL detection\n",
    "*   API-based GoIndex crawler\n",
    "*   Parallel/Multiple files downloader\n",
    "\n",
    "**Version 2**:\n",
    "\n",
    "\t17 April 2020 (v2.1)\n",
    "\n",
    "\t+ fixed URL duplicated when crawling\n",
    "\t+ added search 'files' key for some websites do not have proper files structure. So, we search it\n",
    "\n",
    "\t16 April 2020\n",
    "\n",
    "\t+ crawler_v2:\n",
    "\t\t* API-based GoIndex crawler\n",
    "\t\t* Collecting all urls to be downloaded\n",
    "\t+ parallel downloader\n",
    "\t\t* TDQM progress bar\n",
    "\n",
    "**Version 1**:\n",
    "\n",
    "\t15 April 2020\n",
    "\t-   Added auto-domain URL detection\n",
    "\t-   Added simple download queue\n",
    "\n",
    "\t14 April 2020\n",
    "\t\t-   initial"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SHZKD2eIrSWC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Mounting Google Drive, ignore this section if you don't want to \n",
    "# save on your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ei98hPvrrlNb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Install dependencies\n",
    "!pip install requests tqdm"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ki6wS0MCrnqC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Import dependencies\n",
    "\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from urllib import parse\n",
    "\n",
    "import requests\n",
    "import tqdm"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ew87GUTHUu2p",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "SHOW_DOWNLOAD_PROGRESS = False\n",
    "OVERWRITE = True\n",
    "\n",
    "MIN_DELAY = 3\n",
    "MAX_DELAY = 5\n",
    "\n",
    "\n",
    "def check_exclusion(name, exclusions):\n",
    "    for exc in exclusions:\n",
    "        if exc in name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find(key, dictionary):\n",
    "    for k, v in dictionary.items():\n",
    "        if k == key:\n",
    "            yield v\n",
    "        elif isinstance(v, dict):\n",
    "            for result in find(key, v):\n",
    "                yield result\n",
    "        elif isinstance(v, list):\n",
    "            for d in v:\n",
    "                for result in find(key, d):\n",
    "                    yield result\n",
    "\n",
    "\n",
    "def crawler_v2(url, downloading_dict, path, level, exclusions, verbose=False):\n",
    "    # let slow down a bit\n",
    "    sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "\n",
    "    url = parse.urlparse(url)\n",
    "    print(url.geturl())\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url.geturl(), data={})\n",
    "        response_json = json.loads(response.text)\n",
    "    except JSONDecodeError:\n",
    "        print('- Data is missing! change a plan -')\n",
    "        print('- > use terminal CURL            -')\n",
    "        response = os.popen(\"curl {} -d ''\".format(url.geturl())).read()\n",
    "        response_json = json.loads(response)\n",
    "    else:\n",
    "        print('Nah, something went wrong!')\n",
    "        return []\n",
    "\n",
    "    files_dict = list(find('files', response_json))[0]\n",
    "\n",
    "    for file in files_dict:\n",
    "        name = file['name']\n",
    "\n",
    "        # if @name contains exclusion word, we ignore\n",
    "        if check_exclusion(name, exclusions):\n",
    "            continue\n",
    "\n",
    "        if 'folder' in file['mimeType']:\n",
    "            next_url = url.geturl() + parse.quote(name) + \"/\"\n",
    "            next_path = os.path.join(path, name)\n",
    "            downloading_dict = crawler_v2(next_url, downloading_dict, next_path, level + 1, exclusions, verbose)\n",
    "        else:\n",
    "            name = file['name']\n",
    "            if verbose:\n",
    "                print('  ' + name)\n",
    "            downloading_dict.append({\n",
    "                'folder': path,\n",
    "                'filename': name,\n",
    "                'filename_abs': os.path.join(path, name),\n",
    "                'size': file['size'],\n",
    "                'url': url.geturl() + parse.quote(name),\n",
    "            })\n",
    "\n",
    "    # print(json.dumps(downloading_dict, indent=2), end='\\n\\n')\n",
    "    return downloading_dict\n",
    "\n",
    "\n",
    "def download_agent(task, OVERWRITE=OVERWRITE):\n",
    "    # Making multiple requests too quick can cause yourself banned, so let set random delay (1, 10)\n",
    "    sleep(randint(MIN_DELAY, MAX_DELAY))\n",
    "\n",
    "    folder = task['folder']\n",
    "    filename_abs = task['filename_abs']\n",
    "    url = task['url']\n",
    "\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    if os.path.exists(filename_abs) or OVERWRITE:\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code is not 200:\n",
    "            return 1\n",
    "        with open(filename_abs, 'ab+') as f:\n",
    "            f.write(r.content)\n",
    "    return 0"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nqwqZ8Qrr-6m",
    "colab_type": "code",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "MAX_DOWNLOAD_TASKS = 4\n",
    "exclusions = ['__MACOSX/']\n",
    "\n",
    "destination = \"/content/drive/My Drive/Knowledge/_Trainings/_download\"\n",
    "download_tasks = [\n",
    "    {\n",
    "        'folder': 'FrontEndMasters - Complete Intro to Containers',\n",
    "        'url': 'https://tutnetflix.mlwdl.workers.dev/FrontEndMasters%20-%20Complete%20Intro%20to%20Containers/'\n",
    "    },\n",
    "    {\n",
    "        'folder': 'test',\n",
    "        'url': 'https://gdrv.icu/0:/Star/宮崎あや/2013-2016/'\n",
    "    },\n",
    "]\n",
    "\n",
    "print('##################################')\n",
    "print('# Crawling all downloadable urls #')\n",
    "print('##################################', end='\\n\\n')\n",
    "tasks = []\n",
    "for task in download_tasks:\n",
    "    tasks += crawler_v2(task['url'], [], os.path.join(destination, task['folder']), 0, exclusions, verbose=False)\n",
    "    # print(json.dumps(tasks, indent=2), end='\\n\\n')\n",
    "\n",
    "# print(json.dumps(tasks, indent=2))\n",
    "print('\\nCollecting', len(tasks), 'is completed', end='\\n\\n')\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZjpVT5B_to7l",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print('##################################')\n",
    "print('# Downloading files and folders  #')\n",
    "print('##################################', end='\\n\\n')\n",
    "pool = multiprocessing.Pool(processes=MAX_DOWNLOAD_TASKS)  # Num of CPUs\n",
    "\n",
    "failures = 0\n",
    "tasks_list = [task.get('filename') for task in tasks]\n",
    "with tqdm.tqdm(total=len(tasks)) as pbar:\n",
    "    for i, result in enumerate(pool.imap_unordered(download_agent, tasks)):\n",
    "        pbar.set_description('Downloading %s' % tasks_list[i])\n",
    "        failures += result\n",
    "        pbar.update()\n",
    "print('\\nTotal number of download failures:', failures)\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "\n",
    "print('\\nAll done, Voila!')"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}