{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copy of Recursive-GoIndex-Downloader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH7ZlsGKsQ0X",
    "colab_type": "text"
   },
   "source": [
    "## Recursive GoIndex Downloader by atlonxp\n",
    "\n",
    "This code was created and improved by adapting the code from pankaj260 https://colab.research.google.com/drive/1tmsLGuswIZIZ_oM35EMW8TbJ6pQPt1rY#scrollTo=3bCnUMUg_SoT&forceEdit=true&sandboxMode=true\n",
    "\n",
    "**Features**\n",
    "*   Recursive crawler (atlonxp)\n",
    "*   Download all folders and files in a given url (atlonxp)\n",
    "*   Download all folders and files in in sub-folders (atlonxp)\n",
    "*   Adaptive delay in fetching url (atlonxp)\n",
    "*   Store folders/files directly to your Google Drive (pankaj260)\n",
    "*   Folders and files exclusion filters\n",
    "*   Download queue supported\n",
    "*   Auto-domain URL detection\n",
    "\n",
    "**Coming soon**\n",
    "-   TDQM multiple/paralelled downloader\n",
    "-   Aria2 integration\n",
    "\n",
    "\n",
    "**Changelogs**\n",
    "\n",
    "15 April 2020:\n",
    "-   Added auto-domain URL detection\n",
    "-   Added simple download queue\n",
    "\n",
    "14 April 2020:\n",
    "-   initial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SHZKD2eIrSWC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Mounting Google Drive, ignore this section if you don't want to \n",
    "# save on your Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ei98hPvrrlNb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Install dependencies\n",
    "\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!pip install selenium bs4 requests"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ki6wS0MCrnqC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gda7vWOuryx0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Functions\n",
    "\n",
    "def get_page_source(url, browser, delay, level, verbose=False):\n",
    "    try:\n",
    "        if verbose:\n",
    "            print('\\t' * level, url)\n",
    "        browser.get(url)\n",
    "        list_elm = WebDriverWait(browser, delay).until(lambda x: x.find_element_by_xpath('//*[@id=\"list\"]/li[1]'))\n",
    "        return browser.page_source\n",
    "    except TimeoutException:\n",
    "        raise Exception('Unable to load this website')\n",
    "\n",
    "\n",
    "def check_exclusion(url, exclusions):\n",
    "    for exc in exclusions:\n",
    "        if exc in url:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def crawler(url, downloading_dict, level, exclusions, max_delay=60, verbose=False):\n",
    "    domain_match = re.search('(https?://[A-Za-z_0-9.-]+).*', url)\n",
    "    if not domain_match:\n",
    "        raise Exception('Wrong format URL')\n",
    "      \n",
    "    domain_url = domain_match.group(1)\n",
    "\n",
    "    page_source = get_page_source(url, browser, max_delay, level, verbose)\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    \n",
    "    for fileFolders in soup.select('li.mdui-list-item a'):\n",
    "        if (fileFolders.contents[1].text.find(\"folder_open\") > -1):\n",
    "            tempFolderName = fileFolders.contents[1].text\n",
    "            folderName = tempFolderName.replace(\"folder_open\", \"\").strip().strip().replace(\"/\", \"\")\n",
    "            folder_url = domain_url + fileFolders['href']\n",
    "\n",
    "            if not check_exclusion(folder_url, exclusions):\n",
    "                downloading_dict[folderName] = {\n",
    "                    'type': 'folder',\n",
    "                    'url': folder_url,\n",
    "                    'child': crawler(folder_url, {}, level + 1, exclusions)\n",
    "                }\n",
    "        else:\n",
    "            tempFileName = fileFolders.contents[1].text\n",
    "            fileName = tempFileName.replace(\"insert_drive_file\", \"\").strip().replace(\"/\", \"\")\n",
    "            file_url = domain_url + fileFolders['href'].replace(\"?a=view\", \"\")\n",
    "\n",
    "            if not check_exclusion(file_url, exclusions):\n",
    "                downloading_dict[fileName] = {\n",
    "                    'type': 'file',\n",
    "                    'url': file_url\n",
    "                }\n",
    "\n",
    "    return downloading_dict\n",
    "\n",
    "\n",
    "def downloader(downloading_dict, level, path, verbose=False):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for key, value in downloading_dict.items():\n",
    "        if value['type'] == 'folder':\n",
    "            path_abs_dir = os.path.join(path, key)\n",
    "            if verbose:\n",
    "                print('\\t' * level, path_abs_dir)\n",
    "\n",
    "            try:\n",
    "                if not os.path.exists(path_abs_dir):\n",
    "                    os.mkdir(path_abs_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "            downloader(value['child'], level + 1, path_abs_dir, verbose)\n",
    "        else:\n",
    "            file_abs_path = os.path.join(path, key)\n",
    "            if verbose:\n",
    "                print('\\t' * level, file_abs_path)\n",
    "            if os.path.exists(file_abs_path):\n",
    "                print(\"skipping => \" + os.path.join(path, key))\n",
    "            else:\n",
    "                r = requests.get(value['url'], stream=True)\n",
    "                with open(file_abs_path, \"ab+\") as f:\n",
    "                    f.write(r.content)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JAM_iv_axZc7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Browser Initialization\n",
    "\n",
    "chromedriver = \"/usr/bin/chromedriver\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "browser = webdriver.Chrome(chromedriver, options=chrome_options)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nqwqZ8Qrr-6m",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "exclusions = ['__MACOSX/']\n",
    "max_waiting_delay = 60\n",
    "\n",
    "destination = \"/content/drive/My Drive/Knowledge/_Trainings/_download/\"\n",
    "download_tasks = [\n",
    "    {\n",
    "        'folder': 'ABC',\n",
    "        'url': 'https://xxx.workers.dev/abc/'\n",
    "    },\n",
    "    {\n",
    "        'folder': 'DEF',\n",
    "        'url': 'https://xxx.workers.dev/def/'\n",
    "    },\n",
    "]\n",
    "\n",
    "for task in download_tasks:\n",
    "    print('Task: ', task['folder'])\n",
    "    downloading_dict = crawler(task['url'], {}, 0, exclusions, verbose=True)\n",
    "    downloader(downloading_dict, 0, path=os.path.join(destination, task['folder']), verbose=True)\n",
    "    print('Task completed --------------------', end='\\n\\n')\n",
    "\n",
    "print('All done, Voila!')"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}